# Fennec Environment Configuration
# Copy this file to .env and fill in your actual API keys
# DO NOT commit the .env file - it's already in .gitignore

# =============================================================================
# PROVIDER CONFIGURATION
# =============================================================================
# Fennec supports multiple AI providers. Configure the ones you plan to use.
# Only the keys for your active provider(s) are required.

# -----------------------------------------------------------------------------
# OpenAI (Default Provider)
# -----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# Optional: Override default model (default: gpt-4)
# OPENAI_MODEL=gpt-4-turbo-preview
# OPENAI_MODEL=gpt-3.5-turbo

# Optional: Override base URL (for proxies or custom endpoints)
# OPENAI_BASE_URL=https://api.openai.com/v1

# Optional: Request timeout in seconds (default: 30)
# OPENAI_TIMEOUT=60

# Optional: Organization ID (if applicable)
# OPENAI_ORGANIZATION=org-your-org-id

# -----------------------------------------------------------------------------
# Anthropic (Claude)
# -----------------------------------------------------------------------------
# Get your API key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Optional: Override default model
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# ANTHROPIC_MODEL=claude-3-opus-20240229
# ANTHROPIC_MODEL=claude-3-haiku-20240307

# Optional: Override base URL
# ANTHROPIC_BASE_URL=https://api.anthropic.com

# Optional: Request timeout in seconds (default: 30)
# ANTHROPIC_TIMEOUT=60

# -----------------------------------------------------------------------------
# Azure OpenAI
# -----------------------------------------------------------------------------
# Get credentials from: https://portal.azure.com/
# AZURE_OPENAI_API_KEY=your-azure-api-key-here
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_DEPLOYMENT=your-deployment-name
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# -----------------------------------------------------------------------------
# OpenRouter (Multiple Models via Single API)
# -----------------------------------------------------------------------------
# Get your API key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-your-openrouter-api-key-here

# Optional: Override default model
# OPENROUTER_MODEL=anthropic/claude-3.5-sonnet
# OPENROUTER_MODEL=openai/gpt-4-turbo
# OPENROUTER_MODEL=meta-llama/llama-3.1-405b-instruct

# Optional: Your app URL (for rankings)
# OPENROUTER_APP_URL=https://your-app.com

# Optional: Your app name (for rankings)
# OPENROUTER_APP_NAME=Fennec

# -----------------------------------------------------------------------------
# Ollama (Local Models)
# -----------------------------------------------------------------------------
# Ollama runs models locally - no API key needed
# Just ensure Ollama is running: ollama serve

# Optional: Override default host (default: http://localhost:11434)
# OLLAMA_HOST=http://localhost:11434

# Optional: Override default model
# OLLAMA_MODEL=llama3.1:latest
# OLLAMA_MODEL=codellama:latest
# OLLAMA_MODEL=mistral:latest

# Optional: Request timeout in seconds (default: 120 for local inference)
# OLLAMA_TIMEOUT=180

# -----------------------------------------------------------------------------
# Together.ai
# -----------------------------------------------------------------------------
# Get your API key from: https://api.together.xyz/settings/api-keys
# TOGETHER_API_KEY=your-together-api-key-here

# Optional: Override default model
# TOGETHER_MODEL=meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
# TOGETHER_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1

# =============================================================================
# GENERAL SETTINGS
# =============================================================================

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
# Set the logging level for Rust tracing
# Options: trace, debug, info, warn, error (default: info)
RUST_LOG=info

# Optional: Enable more verbose logging for specific crates
# RUST_LOG=fennec=debug,fennec_provider=trace,fennec_commands=debug

# Optional: Enable backtraces for debugging (0=off, 1=on, full=verbose)
# RUST_BACKTRACE=1

# -----------------------------------------------------------------------------
# Fennec Configuration
# -----------------------------------------------------------------------------
# Optional: Override default sandbox level
# Options: read-only, workspace-write, danger-full-access
# FENNEC_SANDBOX_LEVEL=workspace-write

# Optional: Override default provider
# Options: openai, anthropic, azure, openrouter, ollama, together
# FENNEC_PROVIDER=openai

# Optional: Enable/disable audit logging (default: true)
# FENNEC_AUDIT_ENABLED=true

# Optional: Custom audit log path
# FENNEC_AUDIT_PATH=/path/to/audit.jsonl

# Optional: Custom memory storage path (default: .fennec)
# FENNEC_MEMORY_PATH=.fennec

# Optional: Maximum transcript size in bytes (default: 10000)
# FENNEC_MAX_TRANSCRIPT_SIZE=10000

# Optional: Enable AGENTS.md integration (default: true)
# FENNEC_ENABLE_AGENTS_MD=true

# -----------------------------------------------------------------------------
# Development & Testing
# -----------------------------------------------------------------------------
# Optional: Enable test mode (skips certain validations)
# FENNEC_TEST_MODE=false

# Optional: Mock provider responses (for testing without API calls)
# FENNEC_MOCK_PROVIDER=false

# Optional: Enable debug UI features
# FENNEC_DEBUG_UI=false

# =============================================================================
# NOTES
# =============================================================================
# 1. Copy this file: cp .env.example .env
# 2. Fill in your API keys for the providers you want to use
# 3. NEVER commit your .env file (it's already in .gitignore)
# 4. Use environment-specific files: .env.development, .env.production
# 5. Variables can also be set in ~/.config/fennec/config.toml
# 6. Command-line flags override environment variables
